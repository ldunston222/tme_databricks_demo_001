{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e24f0476",
   "metadata": {},
   "source": [
    "## Section 1: Install Dependencies\n",
    "\n",
    "Install required packages for the AI Workflow Evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ef205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install required packages\n",
    "packages = [\"pyspark>=3.3.0\", \"mlflow>=2.0.0\", \"pandas>=1.5.0\", \"numpy>=1.23.0\"]\n",
    "\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c9985",
   "metadata": {},
   "source": [
    "## Section 2: Import Required Libraries\n",
    "\n",
    "Import PySpark, MLflow, Pandas, and other necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238929a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')  # Add parent directory to path to import evaluator module\n",
    "\n",
    "import mlflow\n",
    "import mlflow.entities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Import from local evaluator module\n",
    "from evaluator import Episode, EpisodeEvaluator, MetricsTracker, validate_episode\n",
    "from evaluator.invariants import assert_idempotent, assert_low_drift\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76077b6",
   "metadata": {},
   "source": [
    "## Section 3: Initialize MLflow Tracking\n",
    "\n",
    "Configure MLflow for local tracking (can be changed to Databricks tracking URI on a Databricks cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MLflow tracking\n",
    "# For Databricks: mlflow.set_tracking_uri(\"databricks\")\n",
    "# For local MLflow server: mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "# For local file system (default):\n",
    "mlflow.set_tracking_uri(None)  # Uses local ./mlruns directory\n",
    "\n",
    "# Create experiment for episode evaluation\n",
    "experiment_name = \"episode_evaluation_demo\"\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "except:\n",
    "    # Experiment already exists\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"✓ MLflow configured - Experiment: {experiment_name}\")\n",
    "print(f\"  Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e4e642",
   "metadata": {},
   "source": [
    "## Section 4: Create Sample Episodes with Unique IDs\n",
    "\n",
    "Define multiple episodes (membranes) with unique IDs, inputs, expected outputs, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample episodes - each represents a unique AI workflow membrane\n",
    "\n",
    "episode_1 = Episode(\n",
    "    episode_id=\"ep-001-geography\",\n",
    "    inputs={\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"context\": \"European capitals\",\n",
    "        \"temperature\": 0.7\n",
    "    },\n",
    "    expected_outputs={\n",
    "        \"answer\": \"Paris\",\n",
    "        \"confidence\": 0.95,\n",
    "        \"reasoning\": \"Paris is the most well-known capital of France\"\n",
    "    },\n",
    "    prompt=\"Answer the geography question based on context. Provide reasoning.\",\n",
    "    model_name=\"gpt-4\",\n",
    "    token_counts={\"input_tokens\": 45, \"output_tokens\": 32}\n",
    ")\n",
    "\n",
    "episode_2 = Episode(\n",
    "    episode_id=\"ep-002-math\",\n",
    "    inputs={\n",
    "        \"operation\": \"multiply\",\n",
    "        \"a\": 7,\n",
    "        \"b\": 8\n",
    "    },\n",
    "    expected_outputs={\n",
    "        \"result\": 56,\n",
    "        \"confidence\": 1.0\n",
    "    },\n",
    "    prompt=\"Perform the requested mathematical operation\",\n",
    "    model_name=\"gpt-4\",\n",
    "    token_counts={\"input_tokens\": 25, \"output_tokens\": 15}\n",
    ")\n",
    "\n",
    "episode_3 = Episode(\n",
    "    episode_id=\"ep-003-sentiment\",\n",
    "    inputs={\n",
    "        \"text\": \"I absolutely love this product! It works perfectly.\",\n",
    "        \"model\": \"sentiment-analyzer\"\n",
    "    },\n",
    "    expected_outputs={\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"score\": 0.92\n",
    "    },\n",
    "    prompt=\"Analyze the sentiment of the given text\",\n",
    "    model_name=\"bert-sentiment\",\n",
    "    token_counts={\"input_tokens\": 35, \"output_tokens\": 12}\n",
    ")\n",
    "\n",
    "episodes = [episode_1, episode_2, episode_3]\n",
    "\n",
    "print(\"✓ Created 3 sample episodes (membranes)\")\n",
    "for ep in episodes:\n",
    "    print(f\"  - {ep.episode_id}: {ep.prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96e1eb",
   "metadata": {},
   "source": [
    "## Section 5: Create DataFrame of Episodes\n",
    "\n",
    "Convert episodes into a Pandas DataFrame for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of episodes for batch processing\n",
    "episode_data = {\n",
    "    \"episode_id\": [ep.episode_id for ep in episodes],\n",
    "    \"model_name\": [ep.model_name for ep in episodes],\n",
    "    \"prompt\": [ep.prompt for ep in episodes],\n",
    "    \"token_count\": [ep.token_counts[\"input_tokens\"] + ep.token_counts[\"output_tokens\"] for ep in episodes]\n",
    "}\n",
    "\n",
    "df_episodes = pd.DataFrame(episode_data)\n",
    "print(\"✓ Episode DataFrame created:\")\n",
    "print(df_episodes)\n",
    "print(f\"\\nDataFrame shape: {df_episodes.shape}\")\n",
    "print(f\"Columns: {list(df_episodes.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d31c1",
   "metadata": {},
   "source": [
    "## Section 6: Execute Episodes as Spark Jobs\n",
    "\n",
    "Simulate Spark job execution by executing episodes and capturing outputs (in a real scenario, these would be actual Spark RDD/DataFrame operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cae6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_episode_job(episode: Episode) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simulate Spark job execution for an episode.\n",
    "    In production, this would be actual Spark RDD/DataFrame operations.\n",
    "    \n",
    "    Returns the output from the executed episode.\n",
    "    \"\"\"\n",
    "    # Simulate different execution scenarios\n",
    "    if episode.episode_id == \"ep-001-geography\":\n",
    "        # Perfect match scenario\n",
    "        return {\n",
    "            \"answer\": \"Paris\",\n",
    "            \"confidence\": 0.95,\n",
    "            \"reasoning\": \"Paris is the most well-known capital of France\"\n",
    "        }\n",
    "    elif episode.episode_id == \"ep-002-math\":\n",
    "        # Perfect match scenario\n",
    "        return {\n",
    "            \"result\": 56,\n",
    "            \"confidence\": 1.0\n",
    "        }\n",
    "    elif episode.episode_id == \"ep-003-sentiment\":\n",
    "        # Slight drift scenario - different confidence score\n",
    "        return {\n",
    "            \"sentiment\": \"positive\",\n",
    "            \"score\": 0.89  # Slightly different from expected 0.92\n",
    "        }\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Execute episodes and capture outputs\n",
    "print(\"Executing episodes as Spark jobs...\\n\")\n",
    "episode_outputs = []\n",
    "\n",
    "for episode in episodes:\n",
    "    actual_output = execute_episode_job(episode)\n",
    "    episode_outputs.append((episode, actual_output))\n",
    "    print(f\"Episode: {episode.episode_id}\")\n",
    "    print(f\"  Expected: {episode.expected_outputs}\")\n",
    "    print(f\"  Actual:   {actual_output}\")\n",
    "    print()\n",
    "\n",
    "print(\"✓ All episodes executed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd25370",
   "metadata": {},
   "source": [
    "## Section 7: Evaluate Idempotence Results\n",
    "\n",
    "Compare actual outputs against expected outputs to determine idempotency status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa953aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the episode evaluator\n",
    "evaluator = EpisodeEvaluator()\n",
    "\n",
    "# Evaluate all episodes\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATING EPISODE IDEMPOTENCY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for episode, actual_output in episode_outputs:\n",
    "    # Validate episode first\n",
    "    try:\n",
    "        validate_episode(episode)\n",
    "        validation_status = \"✓ Valid\"\n",
    "    except Exception as e:\n",
    "        validation_status = f\"✗ Invalid: {e}\"\n",
    "    \n",
    "    # Evaluate idempotency\n",
    "    match_result, metrics = evaluator.evaluate_episode(episode, actual_output)\n",
    "    \n",
    "    # Interpret match result\n",
    "    if match_result == 1.0:\n",
    "        status = \"✓ MATCH (Idempotent)\"\n",
    "    elif match_result == 0.0:\n",
    "        status = \"✗ MISMATCH\"\n",
    "    else:\n",
    "        status = \"? UNDETERMINED (Partial)\"\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        \"episode_id\": episode.episode_id,\n",
    "        \"match_result\": match_result,\n",
    "        \"status\": status,\n",
    "        \"drift\": metrics[\"drift\"],\n",
    "        \"coherence\": metrics[\"coherence\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"Episode: {episode.episode_id}\")\n",
    "    print(f\"  Validation: {validation_status}\")\n",
    "    print(f\"  Status: {status}\")\n",
    "    print(f\"  Match Score: {match_result:.2f}\")\n",
    "    print(f\"  Drift Score: {metrics['drift']:.4f}\")\n",
    "    print(f\"  Coherence Score: {metrics['coherence']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(evaluation_results)\n",
    "print(\"\\n✓ Evaluation Results Summary:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3323b9",
   "metadata": {},
   "source": [
    "## Section 8: Track Metrics with MLflow\n",
    "\n",
    "Each episode evaluation automatically logs to MLflow. View the metrics in the MLflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849299fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query MLflow runs for this experiment\n",
    "print(\"=\" * 60)\n",
    "print(\"MLFLOW TRACKING - Logged Runs\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "print(f\"Total runs logged: {len(runs)}\\n\")\n",
    "\n",
    "for idx, run in runs.iterrows():\n",
    "    run_id = run[\"run_id\"]\n",
    "    metrics = run.get([\"metrics.idempotency_score\", \"metrics.drift_score\", \"metrics.coherence_score\"], default=None)\n",
    "    \n",
    "    print(f\"Run {idx + 1}: {run_id[:8]}...\")\n",
    "    print(f\"  Episode ID: {run['params.episode_id']}\")\n",
    "    print(f\"  Idempotency Score: {run.get('metrics.idempotency_score', 'N/A')}\")\n",
    "    print(f\"  Drift Score: {run.get('metrics.drift_score', 'N/A')}\")\n",
    "    print(f\"  Coherence Score: {run.get('metrics.coherence_score', 'N/A')}\")\n",
    "    print()\n",
    "\n",
    "print(\"✓ MLflow tracking active and metrics logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd547381",
   "metadata": {},
   "source": [
    "## Section 9: Accumulate Metrics Across Runs\n",
    "\n",
    "Show how metrics accumulate across multiple executions of the same episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ccb64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ACCUMULATING METRICS ACROSS RUNS\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Show accumulated metrics for each episode\n",
    "for episode in episodes:\n",
    "    summary = episode.get_metrics_summary()\n",
    "    print(f\"Episode: {episode.episode_id}\")\n",
    "    print(f\"  Execution Count: {summary['execution_count']}\")\n",
    "    print(f\"  Last Execution: {summary.get('last_execution_at', 'N/A')}\")\n",
    "    \n",
    "    if summary['execution_count'] > 0:\n",
    "        print(f\"  Match Rate: {summary.get('match_rate', 0):.2%}\")\n",
    "        print(f\"  Avg Drift: {summary.get('avg_drift', 0):.4f}\")\n",
    "        print(f\"  Avg Coherence: {summary.get('avg_coherence', 0):.4f}\")\n",
    "        print(f\"  Drift Stdev: {summary.get('drift_stdev', 0):.4f}\")\n",
    "        print(f\"  Coherence Stdev: {summary.get('coherence_stdev', 0):.4f}\")\n",
    "        print(f\"  Raw Metrics:\")\n",
    "        print(f\"    Match: {episode.metrics['match']}\")\n",
    "        print(f\"    Drift: {episode.metrics['drift']}\")\n",
    "        print(f\"    Coherence: {episode.metrics['coherence']}\")\n",
    "    print()\n",
    "\n",
    "print(\"✓ Metrics accumulated successfully\")\n",
    "\n",
    "# Simulate running episodes again to accumulate more data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RE-EXECUTING EPISODES FOR SECOND RUN\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "for episode, actual_output in episode_outputs:\n",
    "    # Re-evaluate (simulates another run)\n",
    "    match_result, metrics = evaluator.evaluate_episode(episode, actual_output)\n",
    "    print(f\"Episode {episode.episode_id}: Match={match_result}, Drift={metrics['drift']:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics after second run:\")\n",
    "for episode in episodes:\n",
    "    summary = episode.get_metrics_summary()\n",
    "    print(f\"{episode.episode_id}: Executions={summary['execution_count']}, Match Rate={summary.get('match_rate', 0):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4dafa",
   "metadata": {},
   "source": [
    "## Section 10: Reset Metrics Function\n",
    "\n",
    "Demonstrate resetting accumulated metrics for fresh evaluation cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6354555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RESETTING METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Show metrics before reset\n",
    "print(\"Before reset:\")\n",
    "for episode in episodes:\n",
    "    summary = episode.get_metrics_summary()\n",
    "    print(f\"  {episode.episode_id}: Executions={summary['execution_count']}\")\n",
    "\n",
    "print(\"\\nResetting all episode metrics...\")\n",
    "for episode in episodes:\n",
    "    episode.reset_metrics()\n",
    "\n",
    "print(\"\\nAfter reset:\")\n",
    "for episode in episodes:\n",
    "    summary = episode.get_metrics_summary()\n",
    "    print(f\"  {episode.episode_id}: Executions={summary['execution_count']}\")\n",
    "    print(f\"    Match history: {episode.metrics['match']}\")\n",
    "    print(f\"    Drift history: {episode.metrics['drift']}\")\n",
    "\n",
    "print(\"\\n✓ Metrics reset successfully - ready for fresh evaluation cycle\")\n",
    "\n",
    "# Batch evaluation summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BATCH EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "batch_result = evaluator.evaluate_batch(episode_outputs, batch_id=\"demo_batch_001\")\n",
    "print(f\"Batch ID: {batch_result['batch_id']}\")\n",
    "print(f\"Episodes Processed: {batch_result['episodes_count']}\")\n",
    "print(f\"\\nAggregate Metrics:\")\n",
    "print(f\"  Idempotency Rate: {batch_result['summary']['idempotency_rate']:.2%}\")\n",
    "print(f\"  Average Drift: {batch_result['summary']['avg_drift']:.4f}\")\n",
    "print(f\"  Average Coherence: {batch_result['summary']['avg_coherence']:.4f}\")\n",
    "print(f\"  Total Tokens: {batch_result['summary']['total_tokens']}\")\n",
    "print(f\"  Drift StDev: {batch_result['summary']['drift_stdev']:.4f}\")\n",
    "print(f\"  Coherence StDev: {batch_result['summary']['coherence_stdev']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Demo completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
