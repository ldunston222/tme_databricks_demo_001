{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c4db6b-a1d4-4897-9afa-6276103181b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "de904016"
   },
   "outputs": [],
   "source": [
    "# Observation Deck (Databricks)\n",
    "# Reads from Delta/UC schema: datamodel_db\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "try:\n",
    "    spark\n",
    "except NameError as e:\n",
    "    raise RuntimeError('This notebook must be run on a Spark cluster (Databricks) with an active `spark` session.') from e\n",
    "\n",
    "LOOKBACK_DAYS = 14\n",
    "DRIFT_ALERT_THRESHOLD = 0.20\n",
    "MAX_HEATMAP_EPISODES = 50\n",
    "\n",
    "since_ts = datetime.now(timezone.utc) - timedelta(days=LOOKBACK_DAYS)\n",
    "\n",
    "spark.sql('USE datamodel_db')\n",
    "\n",
    "episodes_df = spark.table('datamodel_db.episode').where(f\"created_at >= TIMESTAMP '{since_ts.isoformat()}'\")\n",
    "steps_df = spark.table('datamodel_db.episode_steps').where(f\"created_at >= TIMESTAMP '{since_ts.isoformat()}'\")\n",
    "eval_df = spark.table('datamodel_db.episode_evaluation').where(f\"evaluated_at >= TIMESTAMP '{since_ts.isoformat()}'\")\n",
    "\n",
    "episodes_df.createOrReplaceTempView('v_episodes')\n",
    "steps_df.createOrReplaceTempView('v_steps')\n",
    "eval_df.createOrReplaceTempView('v_eval')\n",
    "\n",
    "print('âœ“ Views ready:', 'v_episodes', 'v_steps', 'v_eval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5e0224-486b-44e3-a643-e9f014a77586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "f951131d"
   },
   "outputs": [],
   "source": [
    "# Outcomes over time (daily)\n",
    "outcomes = spark.sql('''\n",
    "WITH base AS (\n",
    "  SELECT date_trunc('DAY', created_at) AS day, status\n",
    "  FROM v_episodes\n",
    ")\n",
    "SELECT\n",
    "  day,\n",
    "  SUM(CASE WHEN status = 'successful' THEN 1 ELSE 0 END) AS successful,\n",
    "  SUM(CASE WHEN status = 'degraded' THEN 1 ELSE 0 END) AS degraded,\n",
    "  SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) AS failed,\n",
    "  COUNT(*) AS total,\n",
    "  ROUND(SUM(CASE WHEN status = 'successful' THEN 1 ELSE 0 END) / NULLIF(COUNT(*),0), 4) AS success_rate\n",
    "FROM base\n",
    "GROUP BY day\n",
    "ORDER BY day\n",
    "''')\n",
    "display(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd6428f6-663b-4aa6-8962-6c316499fde3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "435d8606"
   },
   "outputs": [],
   "source": [
    "# Token/cost efficiency\n",
    "eff = spark.sql('''\n",
    "SELECT\n",
    "  status,\n",
    "  COUNT(*) AS episodes,\n",
    "  ROUND(AVG(total_tokens), 2) AS avg_total_tokens,\n",
    "  ROUND(AVG(input_tokens), 2) AS avg_input_tokens,\n",
    "  ROUND(AVG(output_tokens), 2) AS avg_output_tokens,\n",
    "  ROUND(AVG(duration_ms), 1) AS avg_duration_ms,\n",
    "  ROUND(AVG(cost_usd), 6) AS avg_cost_usd,\n",
    "  ROUND(SUM(cost_usd), 6) AS total_cost_usd\n",
    "FROM v_episodes\n",
    "GROUP BY status\n",
    "ORDER BY episodes DESC\n",
    "''')\n",
    "display(eff)\n",
    "\n",
    "cost_per_success = spark.sql('''\n",
    "SELECT\n",
    "  ROUND(SUM(cost_usd) / NULLIF(SUM(CASE WHEN status='successful' THEN 1 ELSE 0 END),0), 6) AS cost_per_successful_episode_usd,\n",
    "  ROUND(SUM(total_tokens) / NULLIF(SUM(CASE WHEN status='successful' THEN 1 ELSE 0 END),0), 2) AS tokens_per_successful_episode\n",
    "FROM v_episodes\n",
    "''')\n",
    "display(cost_per_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24ee2380-9890-46ec-bd8c-3be5779e7cb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fff35786"
   },
   "outputs": [],
   "source": [
    "# Drift vs golden templates\n",
    "# - drift_score is sourced from datamodel_db.episode_evaluation\n",
    "# - episodes marked is_golden=TRUE are treated as the baseline set\n",
    "\n",
    "drift = spark.sql(f'''\n",
    "WITH joined AS (\n",
    "  SELECT\n",
    "    e.created_at,\n",
    "    e.episode_id,\n",
    "    e.golden_template_id,\n",
    "    e.is_golden,\n",
    "    ev.drift_score\n",
    "  FROM v_episodes e\n",
    "  LEFT JOIN v_eval ev\n",
    "    ON e.episode_id = ev.episode_id AND e.episode_version = ev.episode_version\n",
    ")\n",
    "SELECT\n",
    "  date_trunc('DAY', created_at) AS day,\n",
    "  golden_template_id,\n",
    "  ROUND(AVG(CASE WHEN is_golden THEN drift_score END), 4) AS avg_golden_drift,\n",
    "  ROUND(AVG(CASE WHEN NOT is_golden THEN drift_score END), 4) AS avg_non_golden_drift,\n",
    "  SUM(CASE WHEN NOT is_golden AND drift_score IS NOT NULL AND drift_score > {DRIFT_ALERT_THRESHOLD} THEN 1 ELSE 0 END) AS drift_alerts,\n",
    "  SUM(CASE WHEN NOT is_golden THEN 1 ELSE 0 END) AS non_golden_episodes\n",
    "FROM joined\n",
    "GROUP BY day, golden_template_id\n",
    "ORDER BY day, golden_template_id\n",
    "''')\n",
    "display(drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af5a880-5316-4aa8-a85f-bf131d75e3a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "25ffc719"
   },
   "outputs": [],
   "source": [
    "# Step heatmap inputs (use Databricks pivot visualization as heatmap)\n",
    "# Rows: step_name (or step_index)\n",
    "# Columns: episode_id (recent episodes)\n",
    "# Values: total_tokens (or latency_ms / score)\n",
    "\n",
    "recent_episode_ids = [\n",
    "    r.episode_id\n",
    "    for r in spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT episode_id\n",
    "        FROM v_episodes\n",
    "        ORDER BY created_at DESC\n",
    "        LIMIT {MAX_HEATMAP_EPISODES}\n",
    "        \"\"\"\n",
    "    ).collect()\n",
    "]\n",
    "\n",
    "if not recent_episode_ids:\n",
    "    print('No episodes in lookback window.')\n",
    "else:\n",
    "    ids = \",\".join([f\"'{x}'\" for x in recent_episode_ids])\n",
    "    heatmap_long = spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "          COALESCE(step_name, CONCAT('step_', CAST(step_index AS STRING))) AS step_key,\n",
    "          episode_id,\n",
    "          total_tokens,\n",
    "          latency_ms,\n",
    "          score\n",
    "        FROM v_steps\n",
    "        WHERE episode_id IN ({ids})\n",
    "        \"\"\"\n",
    "    )\n",
    "    display(heatmap_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10927ea9-8084-430f-b3e7-2064fe2c7fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "e8b8b3f7"
   },
   "outputs": [],
   "source": [
    "# Failure clustering by step / failure_type / invariant\n",
    "failures = spark.sql('''\n",
    "WITH failed_eps AS (\n",
    "  SELECT episode_id, episode_version, status\n",
    "  FROM v_episodes\n",
    "  WHERE status IN ('degraded', 'failed')\n",
    ")\n",
    "SELECT\n",
    "  s.step_index,\n",
    "  s.step_name,\n",
    "  s.failure_type,\n",
    "  s.invariant_violated,\n",
    "  COUNT(*) AS occurrences\n",
    "FROM v_steps s\n",
    "JOIN failed_eps e\n",
    "  ON s.episode_id = e.episode_id AND s.episode_version = e.episode_version\n",
    "WHERE s.failure_type IS NOT NULL OR s.invariant_violated IS NOT NULL\n",
    "GROUP BY s.step_index, s.step_name, s.failure_type, s.invariant_violated\n",
    "ORDER BY occurrences DESC\n",
    "''')\n",
    "display(failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6701fb-9790-4fd0-ad3c-e1cb5c687d66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8022d57e"
   },
   "outputs": [],
   "source": [
    "# Golden deltas (median non-golden vs median golden)\n",
    "golden_deltas = spark.sql('''\n",
    "WITH joined AS (\n",
    "  SELECT\n",
    "    e.golden_template_id,\n",
    "    e.is_golden,\n",
    "    COALESCE(s.step_name, CONCAT('step_', CAST(s.step_index AS STRING))) AS step_key,\n",
    "    s.total_tokens,\n",
    "    s.latency_ms,\n",
    "    s.score\n",
    "  FROM v_episodes e\n",
    "  JOIN v_steps s\n",
    "    ON e.episode_id = s.episode_id AND e.episode_version = s.episode_version\n",
    "  WHERE e.golden_template_id IS NOT NULL\n",
    "),\n",
    "aggs AS (\n",
    "  SELECT\n",
    "    golden_template_id,\n",
    "    step_key,\n",
    "    percentile_approx(CASE WHEN is_golden THEN total_tokens END, 0.5) AS p50_tokens_golden,\n",
    "    percentile_approx(CASE WHEN NOT is_golden THEN total_tokens END, 0.5) AS p50_tokens_current,\n",
    "    percentile_approx(CASE WHEN is_golden THEN latency_ms END, 0.5) AS p50_latency_golden,\n",
    "    percentile_approx(CASE WHEN NOT is_golden THEN latency_ms END, 0.5) AS p50_latency_current,\n",
    "    percentile_approx(CASE WHEN is_golden THEN score END, 0.5) AS p50_score_golden,\n",
    "    percentile_approx(CASE WHEN NOT is_golden THEN score END, 0.5) AS p50_score_current\n",
    "  FROM joined\n",
    "  GROUP BY golden_template_id, step_key\n",
    ")\n",
    "SELECT\n",
    "  golden_template_id,\n",
    "  step_key,\n",
    "  p50_tokens_golden,\n",
    "  p50_tokens_current,\n",
    "  (p50_tokens_current - p50_tokens_golden) AS delta_tokens,\n",
    "  p50_latency_golden,\n",
    "  p50_latency_current,\n",
    "  (p50_latency_current - p50_latency_golden) AS delta_latency_ms,\n",
    "  p50_score_golden,\n",
    "  p50_score_current,\n",
    "  (p50_score_current - p50_score_golden) AS delta_score\n",
    "FROM aggs\n",
    "ORDER BY golden_template_id, step_key\n",
    "''')\n",
    "display(golden_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49554da4-fc50-4a61-8e08-d6d67ad11f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8fa2bffb"
   },
   "outputs": [],
   "source": [
    "# Single-episode detail (edit EPISODE_ID to view)\n",
    "# This is a timeline/replay view: episode header -> steps -> evaluation(s)\n",
    "\n",
    "EPISODE_ID = None  # e.g. 'ep_demo_001'\n",
    "\n",
    "if not EPISODE_ID:\n",
    "    print('Set EPISODE_ID to view a single episode.')\n",
    "else:\n",
    "    header = spark.sql(f\"SELECT * FROM datamodel_db.episode WHERE episode_id = '{EPISODE_ID}' ORDER BY episode_version DESC\")\n",
    "    steps = spark.sql(f\"SELECT * FROM datamodel_db.episode_steps WHERE episode_id = '{EPISODE_ID}' ORDER BY episode_version DESC, step_index ASC\")\n",
    "    evals = spark.sql(f\"SELECT * FROM datamodel_db.episode_evaluation WHERE episode_id = '{EPISODE_ID}' ORDER BY evaluated_at DESC\")\n",
    "\n",
    "    display(header)\n",
    "    display(steps)\n",
    "    display(evals)\n",
    "\n",
    "    verdict = spark.sql(f\"SELECT status, total_tokens, cost_usd, duration_ms FROM datamodel_db.episode WHERE episode_id = '{EPISODE_ID}' ORDER BY episode_version DESC LIMIT 1\").collect()\n",
    "    if verdict:\n",
    "        v = verdict[0]\n",
    "        print(f\"VERDICT: status={v['status']} | total_tokens={v['total_tokens']} | cost_usd={v['cost_usd']} | duration_ms={v['duration_ms']}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "observation_deck",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
