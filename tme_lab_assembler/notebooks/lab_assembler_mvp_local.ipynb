{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c976c19",
      "metadata": {
        "language": "python",
        "id": "0c976c19"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import date\n",
        "from pathlib import Path\n",
        "\n",
        "# Make the local package importable (Databricks Repos + local dev)\n",
        "sys.path.insert(0, str(Path('..').resolve()))\n",
        "\n",
        "from tme_lab_assembler import auth, persistence, terraform\n",
        "\n",
        "# ---- user inputs ----\n",
        "ENV_NAME = os.getenv('ENV_NAME', 'demo1')\n",
        "CLOUD = os.getenv('CLOUD', 'aws')  # aws|azure|gcp\n",
        "\n",
        "# ---- auth (SSO) ----\n",
        "# The goal is to establish *interactive* user auth for cloud SDKs.\n",
        "# In headless environments, prefer device-code / no-browser flows.\n",
        "AUTH_ENABLED = os.getenv('AUTH_ENABLED', '1') == '1'\n",
        "AWS_PROFILE = os.getenv('AWS_PROFILE', 'default')\n",
        "AWS_SSO_START_URL = os.getenv('AWS_SSO_START_URL')\n",
        "AWS_SSO_REGION = os.getenv('AWS_SSO_REGION')\n",
        "AWS_SSO_ACCOUNT_ID = os.getenv('AWS_SSO_ACCOUNT_ID')\n",
        "AWS_SSO_ROLE_NAME = os.getenv('AWS_SSO_ROLE_NAME')\n",
        "AWS_SSO_NO_BROWSER = os.getenv('AWS_SSO_NO_BROWSER', '1') == '1'\n",
        "AZ_TENANT_ID = os.getenv('AZ_TENANT_ID')\n",
        "\n",
        "# Where to persist artifacts:\n",
        "PERSIST_MODE = os.getenv('PERSIST_MODE', 'dbfs')  # dbfs|table|both\n",
        "ARTIFACT_DBFS_DIR = os.getenv('ARTIFACT_DBFS_DIR', 'dbfs:/FileStore/tme_lab_assembler/artifacts')\n",
        "ARTIFACT_TABLE = os.getenv('ARTIFACT_TABLE', 'tme_lab_assembler.artifacts')\n",
        "\n",
        "# Access placeholders (you can fill these from TF outputs or SDK calls)\n",
        "ACCESS = {\n",
        "    'ssh': '...',\n",
        "    'ui': '...',\n",
        "    'dns': '...',\n",
        "}\n",
        "\n",
        "# Terraform directory relative to this notebook\n",
        "TF_DIR = str(Path('..') / 'infra' / 'terraform' / 'mvp')\n",
        "\n",
        "IS_DATABRICKS = bool(os.environ.get('DATABRICKS_RUNTIME_VERSION'))\n",
        "\n",
        "# Databricks-only globals (optional)\n",
        "try:\n",
        "    dbutils  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    dbutils = None\n",
        "\n",
        "try:\n",
        "    spark  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    spark = None\n",
        "\n",
        "print('IS_DATABRICKS:', IS_DATABRICKS)\n",
        "print('TF_DIR:', Path(TF_DIR).resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671760ca",
      "metadata": {
        "language": "python",
        "id": "671760ca"
      },
      "outputs": [],
      "source": [
        "def terraform_init():\n",
        "    r = terraform.init(TF_DIR)\n",
        "    print(r.stdout)\n",
        "    return r\n",
        "\n",
        "def terraform_apply(env_name: str, cloud: str):\n",
        "    r = terraform.apply(TF_DIR, env_name=env_name, cloud=cloud)\n",
        "    print(r.stdout)\n",
        "    return r\n",
        "\n",
        "def terraform_destroy(env_name: str, cloud: str):\n",
        "    r = terraform.destroy(TF_DIR, env_name=env_name, cloud=cloud)\n",
        "    print(r.stdout)\n",
        "    return r\n",
        "\n",
        "def terraform_outputs_json():\n",
        "    return terraform.output_json(TF_DIR)\n",
        "\n",
        "print('Helpers ready')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd65b18f",
      "metadata": {
        "language": "python",
        "id": "cd65b18f"
      },
      "outputs": [],
      "source": [
        "# Auth (SSO)\n",
        "auth_info = auth.auth_sso(\n",
        "    CLOUD,\n",
        "    enabled=AUTH_ENABLED,\n",
        "    aws_profile=AWS_PROFILE,\n",
        "    aws_sso_start_url=AWS_SSO_START_URL,\n",
        "    aws_sso_region=AWS_SSO_REGION,\n",
        "    aws_sso_account_id=AWS_SSO_ACCOUNT_ID,\n",
        "    aws_sso_role_name=AWS_SSO_ROLE_NAME,\n",
        "    aws_sso_no_browser=AWS_SSO_NO_BROWSER,\n",
        "    az_tenant_id=AZ_TENANT_ID,\n",
        ")\n",
        "auth_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a17e41ed",
      "metadata": {
        "id": "a17e41ed",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Provision\n",
        "terraform_init()\n",
        "terraform_apply(ENV_NAME, CLOUD)\n",
        "tf_outputs = terraform_outputs_json()\n",
        "print('Terraform outputs keys:', list(tf_outputs.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b4b922",
      "metadata": {
        "language": "python",
        "id": "01b4b922"
      },
      "outputs": [],
      "source": [
        "# Build artifact (first handoff record)\n",
        "artifact = {\n",
        "    'env_name': ENV_NAME,\n",
        "    'cloud': CLOUD,\n",
        "    'auth': auth_info,\n",
        "    'resources': tf_outputs,\n",
        "    'access': ACCESS,\n",
        "    'create_at': date.today().isoformat(),\n",
        "}\n",
        "artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "760de03f",
      "metadata": {
        "id": "760de03f",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Persist artifact to DBFS and/or Delta table\n",
        "artifact_dbfs_path = None\n",
        "\n",
        "if PERSIST_MODE in ('dbfs', 'both'):\n",
        "    artifact_dbfs_path = persistence.write_artifact_dbfs(\n",
        "        artifact,\n",
        "        dbfs_dir=ARTIFACT_DBFS_DIR,\n",
        "        dbutils=dbutils,\n",
        "    )\n",
        "    print('Wrote artifact to:', artifact_dbfs_path)\n",
        "\n",
        "if PERSIST_MODE in ('table', 'both'):\n",
        "    if not IS_DATABRICKS:\n",
        "        raise RuntimeError('Delta table persistence requires Spark (Databricks).')\n",
        "    if spark is None:\n",
        "        raise RuntimeError('Spark session not available (spark is undefined).')\n",
        "    persistence.write_artifact_table(\n",
        "        artifact,\n",
        "        spark=spark,\n",
        "        table_name=ARTIFACT_TABLE,\n",
        "        artifact_path=artifact_dbfs_path,\n",
        "    )\n",
        "    print('Appended artifact row to:', ARTIFACT_TABLE)\n",
        "\n",
        "artifact_dbfs_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82360ade",
      "metadata": {
        "language": "python",
        "id": "82360ade"
      },
      "outputs": [],
      "source": [
        "# Cleanup / destroy\n",
        "# - destroys infra via Terraform\n",
        "# - removes the DBFS artifact file and/or deletes Delta rows for env_name\n",
        "\n",
        "def destroy(env_name: str = ENV_NAME, cloud: str = CLOUD, *, dbfs_path: str | None = artifact_dbfs_path):\n",
        "    terraform_destroy(env_name, cloud)\n",
        "    persistence.cleanup_artifact(\n",
        "        env_name=env_name,\n",
        "        dbfs_path=dbfs_path,\n",
        "        table_name=(ARTIFACT_TABLE if PERSIST_MODE in ('table', 'both') else None),\n",
        "        dbutils=dbutils,\n",
        "        spark=spark,\n",
        "    )\n",
        "    print('âœ“ Destroy complete')\n",
        "\n",
        "print('Call destroy() when ready')"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
