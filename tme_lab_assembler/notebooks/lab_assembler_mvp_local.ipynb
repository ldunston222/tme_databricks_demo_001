{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "038612af-9e2e-4bcb-a91b-7089d13ace8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0c976c19",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "# Make the local package importable (Databricks Repos + local dev)\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "\n",
    "from tme_lab_assembler import auth, persistence, terraform\n",
    "\n",
    "# ---- user inputs ----\n",
    "ENV_NAME = os.getenv('ENV_NAME', 'demo1')\n",
    "CLOUD = os.getenv('CLOUD', 'aws')  # aws|azure|gcp\n",
    "\n",
    "# ---- auth (SSO) ----\n",
    "# The goal is to establish *interactive* user auth for cloud SDKs.\n",
    "# In headless environments, prefer device-code / no-browser flows.\n",
    "AUTH_ENABLED = os.getenv('AUTH_ENABLED', '1') == '0'\n",
    "AWS_PROFILE = os.getenv('AWS_PROFILE', 'default')\n",
    "AWS_SSO_START_URL = os.getenv('AWS_SSO_START_URL')\n",
    "AWS_SSO_REGION = os.getenv('AWS_SSO_REGION')\n",
    "AWS_SSO_ACCOUNT_ID = os.getenv('AWS_SSO_ACCOUNT_ID')\n",
    "AWS_SSO_ROLE_NAME = os.getenv('AWS_SSO_ROLE_NAME')\n",
    "AWS_SSO_NO_BROWSER = os.getenv('AWS_SSO_NO_BROWSER', '1') == '1'\n",
    "AZ_TENANT_ID = os.getenv('AZ_TENANT_ID')\n",
    "\n",
    "# Where to persist artifacts:\n",
    "PERSIST_MODE = os.getenv('PERSIST_MODE', 'dbfs')  # dbfs|table|both\n",
    "ARTIFACT_DBFS_DIR = os.getenv('ARTIFACT_DBFS_DIR', 'dbfs:/FileStore/tme_lab_assembler/artifacts')\n",
    "ARTIFACT_TABLE = os.getenv('ARTIFACT_TABLE', 'tme_lab_assembler.artifacts')\n",
    "\n",
    "# Access placeholders (you can fill these from TF outputs or SDK calls)\n",
    "ACCESS = {\n",
    "    'ssh': '...',\n",
    "    'ui': '...',\n",
    "    'dns': '...',\n",
    "}\n",
    "\n",
    "# Terraform directory relative to this notebook\n",
    "TF_DIR = str(Path('..') / 'infra' / 'terraform' / 'mvp')\n",
    "\n",
    "IS_DATABRICKS = bool(os.environ.get('DATABRICKS_RUNTIME_VERSION'))\n",
    "\n",
    "# Databricks-only globals (optional)\n",
    "try:\n",
    "    dbutils  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    dbutils = None\n",
    "\n",
    "try:\n",
    "    spark  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    spark = None\n",
    "\n",
    "print('IS_DATABRICKS:', IS_DATABRICKS)\n",
    "print('TF_DIR:', Path(TF_DIR).resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd42a9b-8fb8-40ce-8fe1-b3f15653e343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "671760ca",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def terraform_init():\n",
    "    r = terraform.init(TF_DIR)\n",
    "    print(r.stdout)\n",
    "    return r\n",
    "\n",
    "def terraform_apply(env_name: str, cloud: str):\n",
    "    r = terraform.apply(TF_DIR, env_name=env_name, cloud=cloud)\n",
    "    print(r.stdout)\n",
    "    return r\n",
    "\n",
    "def terraform_destroy(env_name: str, cloud: str):\n",
    "    r = terraform.destroy(TF_DIR, env_name=env_name, cloud=cloud)\n",
    "    print(r.stdout)\n",
    "    return r\n",
    "\n",
    "def terraform_outputs_json():\n",
    "    return terraform.output_json(TF_DIR)\n",
    "\n",
    "print('Helpers ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8ff3f0-7710-4d0e-ae8d-79b63a75e2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "cd65b18f",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Auth (SSO)\n",
    "auth_info = auth.auth_sso(\n",
    "    CLOUD,\n",
    "    enabled=AUTH_ENABLED,\n",
    "    aws_profile=AWS_PROFILE,\n",
    "    aws_sso_start_url=AWS_SSO_START_URL,\n",
    "    aws_sso_region=AWS_SSO_REGION,\n",
    "    aws_sso_account_id=AWS_SSO_ACCOUNT_ID,\n",
    "    aws_sso_role_name=AWS_SSO_ROLE_NAME,\n",
    "    aws_sso_no_browser=AWS_SSO_NO_BROWSER,\n",
    "    az_tenant_id=AZ_TENANT_ID,\n",
    ")\n",
    "auth_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6104473a-8e4c-4468-95fd-4f0e9be5df31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "a17e41ed",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Provision\n",
    "terraform_init()\n",
    "terraform_apply(ENV_NAME, CLOUD)\n",
    "tf_outputs = terraform_outputs_json()\n",
    "print('Terraform outputs keys:', list(tf_outputs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd929d10-7b7c-4e6d-a8b3-fb1cf2b07b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "01b4b922",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Build artifact (first handoff record)\n",
    "artifact = {\n",
    "    'env_name': ENV_NAME,\n",
    "    'cloud': CLOUD,\n",
    "    'auth': auth_info,\n",
    "    'resources': tf_outputs,\n",
    "    'access': ACCESS,\n",
    "    'create_at': date.today().isoformat(),\n",
    "}\n",
    "artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "558dec16-e0e3-45cb-a366-a937c36f4d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "760de03f",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Persist artifact to DBFS and/or Delta table\n",
    "artifact_dbfs_path = None\n",
    "\n",
    "if PERSIST_MODE in ('dbfs', 'both'):\n",
    "    artifact_dbfs_path = persistence.write_artifact_dbfs(\n",
    "        artifact,\n",
    "        dbfs_dir=ARTIFACT_DBFS_DIR,\n",
    "        dbutils=dbutils,\n",
    "    )\n",
    "    print('Wrote artifact to:', artifact_dbfs_path)\n",
    "\n",
    "if PERSIST_MODE in ('table', 'both'):\n",
    "    if not IS_DATABRICKS:\n",
    "        raise RuntimeError('Delta table persistence requires Spark (Databricks).')\n",
    "    if spark is None:\n",
    "        raise RuntimeError('Spark session not available (spark is undefined).')\n",
    "    persistence.write_artifact_table(\n",
    "        artifact,\n",
    "        spark=spark,\n",
    "        table_name=ARTIFACT_TABLE,\n",
    "        artifact_path=artifact_dbfs_path,\n",
    "    )\n",
    "    print('Appended artifact row to:', ARTIFACT_TABLE)\n",
    "\n",
    "artifact_dbfs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb7a9950-4482-4671-8b3b-9be2f5f73e2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "82360ade",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Cleanup / destroy\n",
    "# - destroys infra via Terraform\n",
    "# - removes the DBFS artifact file and/or deletes Delta rows for env_name\n",
    "\n",
    "def destroy(env_name: str = ENV_NAME, cloud: str = CLOUD, *, dbfs_path: str | None = artifact_dbfs_path):\n",
    "    terraform_destroy(env_name, cloud)\n",
    "    persistence.cleanup_artifact(\n",
    "        env_name=env_name,\n",
    "        dbfs_path=dbfs_path,\n",
    "        table_name=(ARTIFACT_TABLE if PERSIST_MODE in ('table', 'both') else None),\n",
    "        dbutils=dbutils,\n",
    "        spark=spark,\n",
    "    )\n",
    "    print('âœ“ Destroy complete')\n",
    "\n",
    "print('Call destroy() when ready')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab_assembler_mvp_local",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
